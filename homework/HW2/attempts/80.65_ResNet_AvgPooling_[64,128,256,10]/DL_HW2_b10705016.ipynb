{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe4fc70b",
      "metadata": {
        "id": "fe4fc70b"
      },
      "source": [
        "# Assignment 2 -- CNN\n",
        "\n",
        "## Goal\n",
        "\n",
        "1. Learn to design an CNN architecture.\n",
        "\n",
        "2. Learn to implement and realize the batch normalization.\n",
        "\n",
        "3. Learn to implement the pooling layer.\n",
        "\n",
        "4. Learn to implement the residual block.\n",
        "\n",
        "5. Learn to implement the **fully** convolutional residual network without fully connected layers or MLP.\n",
        "\n",
        "\n",
        "## Score\n",
        "\n",
        "1. Part 4: Batch Normalization 20%\n",
        "\n",
        "2. Part 6: Residual Block 20%\n",
        "\n",
        "3. Part 7: Residual Network 20%\n",
        "\n",
        "4. Model size 15%:\n",
        "\n",
        "* 10%: If your model (the number of parameters) is smaller than **6MB**, you will get 10%. Otherwise, no points will be awarded.\n",
        "* 5%:  The remaining 5% will depend on your ranking within the class.\n",
        "\n",
        "5. Model accuracy 15%:\n",
        "\n",
        "* 10%: If your accuracy is higher than **78%**, you will get 10%. Otherwise, no points will be awarded.\n",
        "* 5%:  The remaining 5% will depend on your ranking within the class.\n",
        "\n",
        "6. Model accuracy on another dataset 10%: it will depand on your ranking within the class.\n",
        "\n",
        "## Rule\n",
        "\n",
        "1. Please do NOT call any existing library for your implementations.\n",
        "2. Please do NOT attempt to modify the sections `DO NOT MODIFY`.\n",
        "\n",
        "## Submission\n",
        "\n",
        "Upload your files to NTU Cool.\n",
        "* This .ipynb file: Please rename this file with the format (DL_HW2_StudentID.ipynb)\n",
        "* Model : .pt file\n",
        "* Output: .csv file\n",
        "\n",
        "Deadline: 4/8 midnight (23:59)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a50507e",
      "metadata": {
        "id": "0a50507e"
      },
      "source": [
        "## Task\n",
        "\n",
        "In the following instuction, please design a **fully** convolutional residual network  to label images from The CIFAR-10 dataset. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "![picture](https://drive.google.com/uc?id=1ipIz2kN9fbvaDE1tSXgED3Sthhtyy_Gh)\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "You can find more information on https://www.cs.toronto.edu/~kriz/cifar.html."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d37e4aa",
      "metadata": {
        "id": "5d37e4aa"
      },
      "source": [
        "Please fill your student ID number below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "ce70027f",
      "metadata": {
        "id": "ce70027f"
      },
      "outputs": [],
      "source": [
        "# Please fill your student ID number\n",
        "student_id = 'b10705016'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc0d47f",
      "metadata": {
        "id": "abc0d47f"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Import the necessary libraries\n",
        "\n",
        "`DO NOT MODIFY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "67e5db5d",
      "metadata": {
        "id": "67e5db5d"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.io import loadmat\n",
        "\n",
        "# Optimizer\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "# Pre-processing\n",
        "import torchvision.transforms as trns\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c316e796",
      "metadata": {
        "id": "c316e796"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Global variables.\n",
        "\n",
        "Please keep these hyper-parameters unchange."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "e882625f",
      "metadata": {
        "id": "e882625f"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_classes = 10\n",
        "input_channel = 3\n",
        "num_epoch = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293110f1",
      "metadata": {
        "id": "293110f1"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Create dataloader with pre-processing of dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "f008c646",
      "metadata": {
        "id": "f008c646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9c94f2-be49-4349-ec4c-a4b134048951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Create train/test transforms\n",
        "train_transform = trns.Compose([\n",
        "    trns.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = trns.Compose([\n",
        "    trns.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create train/test datasets with pre-processing\n",
        "# The dataset will automatic download if does not exist\n",
        "data_train = datasets.CIFAR10(root='./dataset/', train=True, transform=train_transform, download=True)\n",
        "data_test = datasets.CIFAR10(root='./dataset/', train=False, transform=test_transform, download=True)\n",
        "\n",
        "# Create train/test dataloader for datasets with  pre-processing\n",
        "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(data_test,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b4d224",
      "metadata": {
        "id": "e8b4d224"
      },
      "source": [
        "## Part 4\n",
        "\n",
        "Please implement batch normalization.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1agPNiE0-YmnmMs711RW52CrH67ETSgPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "f6fd1263",
      "metadata": {
        "id": "f6fd1263"
      },
      "outputs": [],
      "source": [
        "class myBatchNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, eps=1e-4, momentum=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        shape = (1, input_channel, 1, 1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.moving_mean.device != x.device:\n",
        "            self.moving_mean = self.moving_mean.to(x.device)\n",
        "            self.moving_var = self.moving_var.to(x.device)\n",
        "\n",
        "        y, self.moving_mean, self.moving_var = self.batch_norm(\n",
        "            x, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, self.eps, self.momentum)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def batch_norm(self, x, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "        if not torch.is_grad_enabled():\n",
        "            # Use moving averages for inference\n",
        "            x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "        else:\n",
        "            # Compute mean and variance from x (current batch) for training\n",
        "            batch_mean = torch.mean(x, dim=(0, 2, 3), keepdim=True)\n",
        "            batch_var = torch.var(x, dim=(0, 2, 3), keepdim=True, unbiased=False)\n",
        "\n",
        "            # Normalize the input\n",
        "            x_hat = (x - batch_mean) / torch.sqrt(batch_var + eps)\n",
        "\n",
        "            # Update moving averages\n",
        "            moving_mean.data = momentum * moving_mean.data + (1.0 - momentum) * batch_mean.data\n",
        "            moving_var.data = momentum * moving_var.data + (1.0 - momentum) * batch_var.data\n",
        "\n",
        "        # Scale and shift\n",
        "        y = gamma * x_hat + beta\n",
        "\n",
        "        return y, moving_mean, moving_var\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687402f2",
      "metadata": {
        "id": "687402f2"
      },
      "source": [
        "## Part 5\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Basic convolutional layer, activation function, and pooling layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "d8a0d53d",
      "metadata": {
        "id": "d8a0d53d"
      },
      "outputs": [],
      "source": [
        "class myConvolution(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, kernel_size = 1, stride = 1, padding = 0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(input_channel, output_channel, kernel_size, stride, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "bce19e4d",
      "metadata": {
        "id": "bce19e4d"
      },
      "outputs": [],
      "source": [
        "class myActivation(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # ReLU activation function\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.act(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "21df0513",
      "metadata": {
        "id": "21df0513"
      },
      "outputs": [],
      "source": [
        "class myMaxPooling(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size = 2, stride = 2, padding = 0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Max poling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.pool(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "032e1ca8",
      "metadata": {
        "id": "032e1ca8"
      },
      "outputs": [],
      "source": [
        "class myAvgPooling(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size = 2, stride = 2, padding = 0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Average poling layer\n",
        "        self.pool = nn.AvgPool2d(kernel_size, stride, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.pool(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607db462",
      "metadata": {
        "id": "607db462"
      },
      "source": [
        "## Part 6\n",
        "\n",
        "Please implement at least one of the following residual blocks.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1T-prdNyAWnS5qbmxTwt3d6-KxfijiqYW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "e4ca533e",
      "metadata": {
        "id": "e4ca533e"
      },
      "outputs": [],
      "source": [
        "class myResBlock(nn.Module):\n",
        "    def __init__(self, input_channel, med_channel, stride=1, padding=1):\n",
        "        super(myResBlock, self).__init__()\n",
        "        # First Conv -> BN -> ReLU\n",
        "        self.conv1 = myConvolution(input_channel, med_channel, kernel_size=3, stride=stride, padding=padding)\n",
        "        self.bn1 = myBatchNorm(med_channel)\n",
        "        self.relu = myActivation()\n",
        "\n",
        "        # Second Conv -> BN\n",
        "        self.conv2 = myConvolution(med_channel, med_channel, kernel_size=3, stride=1, padding=padding)\n",
        "        self.bn2 = myBatchNorm(med_channel)\n",
        "\n",
        "        # Shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or input_channel != med_channel:\n",
        "            # Adjusting the shortcut to match dimensions if needed\n",
        "            self.shortcut = nn.Sequential(\n",
        "                myConvolution(input_channel, med_channel, kernel_size=1, stride=stride),\n",
        "                myBatchNorm(med_channel)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Adding the identity (skip connection)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dd4d67",
      "metadata": {
        "id": "d5dd4d67"
      },
      "source": [
        "## Part 7\n",
        "\n",
        "Please design your CNN architecture using the implementation of {myBatchNorm, myConvolution, myMaxPooling, myAvgPooling, myActivation, myResBlock}.\n",
        "\n",
        "You have the flexibility to determine the number of layers, the number of hidden neurons in each layer, and the activation function of each layer to design your CNN. Please note that your score will depend on both the size and accuracy of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "7045cab9",
      "metadata": {
        "id": "7045cab9"
      },
      "outputs": [],
      "source": [
        "class myCNN(nn.Module):\n",
        "    def __init__(self, input_channel=3, num_classes=10):\n",
        "        super(myCNN, self).__init__()\n",
        "        # Define the sequence of ResBlocks and Pooling layers\n",
        "        self.layer1 = myResBlock(input_channel, 64)\n",
        "        self.pool1 = myMaxPooling(2, 2)\n",
        "        self.layer2 = myResBlock(64, 128)\n",
        "        self.pool2 = myMaxPooling(2, 2)\n",
        "        self.layer3 = myResBlock(128, 256)\n",
        "        self.pool3 = myMaxPooling(2, 2)\n",
        "        self.layer4 = myResBlock(256, num_classes)\n",
        "        self.avgpool = myAvgPooling((1, 1))  # Global average pooling to reduce spatial dimensions to 1x1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = myCNN(input_channel, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a7d1da",
      "metadata": {
        "id": "41a7d1da"
      },
      "source": [
        "## Part 8\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Multiclass cross-entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "2e16dbca",
      "metadata": {
        "id": "2e16dbca"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1522cb8a",
      "metadata": {
        "id": "1522cb8a"
      },
      "source": [
        "## Part 9\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Mini-batch SGD with momentum and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "3c5df22d",
      "metadata": {
        "id": "3c5df22d"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d20afbce",
      "metadata": {
        "id": "d20afbce"
      },
      "source": [
        "## Part 10\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "008bdc73",
      "metadata": {
        "id": "008bdc73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad1582b-697a-4a22-fa43-505f2a3e02bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch 0 | Batch 0 | Loss 3.9671\n",
            "\tEpoch 0 | Batch 500 | Loss 1.3422\n",
            "\tEpoch 0 | Batch 1000 | Loss 1.1487\n",
            "\tEpoch 0 | Batch 1500 | Loss 1.1664\n",
            "\tEpoch 0 | Batch 2000 | Loss 0.8341\n",
            "\tEpoch 0 | Batch 2500 | Loss 0.7185\n",
            "\tEpoch 0 | Batch 3000 | Loss 0.9120\n",
            "Epoch 0 | Loss 1.2214\n",
            "\tEpoch 1 | Batch 0 | Loss 1.3266\n",
            "\tEpoch 1 | Batch 500 | Loss 0.5072\n",
            "\tEpoch 1 | Batch 1000 | Loss 0.7037\n",
            "\tEpoch 1 | Batch 1500 | Loss 0.9088\n",
            "\tEpoch 1 | Batch 2000 | Loss 0.7123\n",
            "\tEpoch 1 | Batch 2500 | Loss 0.4089\n",
            "\tEpoch 1 | Batch 3000 | Loss 0.7626\n",
            "Epoch 1 | Loss 0.7720\n",
            "\tEpoch 2 | Batch 0 | Loss 0.5769\n",
            "\tEpoch 2 | Batch 500 | Loss 0.8204\n",
            "\tEpoch 2 | Batch 1000 | Loss 0.3895\n",
            "\tEpoch 2 | Batch 1500 | Loss 0.8259\n",
            "\tEpoch 2 | Batch 2000 | Loss 0.4976\n",
            "\tEpoch 2 | Batch 2500 | Loss 0.8203\n",
            "\tEpoch 2 | Batch 3000 | Loss 0.5720\n",
            "Epoch 2 | Loss 0.6107\n",
            "\tEpoch 3 | Batch 0 | Loss 0.9170\n",
            "\tEpoch 3 | Batch 500 | Loss 0.6258\n",
            "\tEpoch 3 | Batch 1000 | Loss 0.7705\n",
            "\tEpoch 3 | Batch 1500 | Loss 1.0135\n",
            "\tEpoch 3 | Batch 2000 | Loss 0.2372\n",
            "\tEpoch 3 | Batch 2500 | Loss 0.2294\n",
            "\tEpoch 3 | Batch 3000 | Loss 0.7346\n",
            "Epoch 3 | Loss 0.4929\n",
            "\tEpoch 4 | Batch 0 | Loss 0.4888\n",
            "\tEpoch 4 | Batch 500 | Loss 0.3764\n",
            "\tEpoch 4 | Batch 1000 | Loss 0.3226\n",
            "\tEpoch 4 | Batch 1500 | Loss 0.8496\n",
            "\tEpoch 4 | Batch 2000 | Loss 0.2087\n",
            "\tEpoch 4 | Batch 2500 | Loss 0.3601\n",
            "\tEpoch 4 | Batch 3000 | Loss 0.7359\n",
            "Epoch 4 | Loss 0.3998\n",
            "\tEpoch 5 | Batch 0 | Loss 0.2633\n",
            "\tEpoch 5 | Batch 500 | Loss 0.2516\n",
            "\tEpoch 5 | Batch 1000 | Loss 0.2286\n",
            "\tEpoch 5 | Batch 1500 | Loss 0.0709\n",
            "\tEpoch 5 | Batch 2000 | Loss 0.3167\n",
            "\tEpoch 5 | Batch 2500 | Loss 0.1758\n",
            "\tEpoch 5 | Batch 3000 | Loss 0.1175\n",
            "Epoch 5 | Loss 0.3210\n",
            "\tEpoch 6 | Batch 0 | Loss 0.1594\n",
            "\tEpoch 6 | Batch 500 | Loss 0.5823\n",
            "\tEpoch 6 | Batch 1000 | Loss 0.1160\n",
            "\tEpoch 6 | Batch 1500 | Loss 0.3692\n",
            "\tEpoch 6 | Batch 2000 | Loss 0.1056\n",
            "\tEpoch 6 | Batch 2500 | Loss 0.2271\n",
            "\tEpoch 6 | Batch 3000 | Loss 0.4658\n",
            "Epoch 6 | Loss 0.2533\n",
            "\tEpoch 7 | Batch 0 | Loss 0.4796\n",
            "\tEpoch 7 | Batch 500 | Loss 0.1709\n",
            "\tEpoch 7 | Batch 1000 | Loss 0.1020\n",
            "\tEpoch 7 | Batch 1500 | Loss 0.0977\n",
            "\tEpoch 7 | Batch 2000 | Loss 0.3146\n",
            "\tEpoch 7 | Batch 2500 | Loss 0.5758\n",
            "\tEpoch 7 | Batch 3000 | Loss 0.4935\n",
            "Epoch 7 | Loss 0.2026\n",
            "\tEpoch 8 | Batch 0 | Loss 0.0268\n",
            "\tEpoch 8 | Batch 500 | Loss 0.2101\n",
            "\tEpoch 8 | Batch 1000 | Loss 0.0494\n",
            "\tEpoch 8 | Batch 1500 | Loss 0.1833\n",
            "\tEpoch 8 | Batch 2000 | Loss 0.1263\n",
            "\tEpoch 8 | Batch 2500 | Loss 0.7434\n",
            "\tEpoch 8 | Batch 3000 | Loss 0.4938\n",
            "Epoch 8 | Loss 0.1591\n",
            "\tEpoch 9 | Batch 0 | Loss 0.0257\n",
            "\tEpoch 9 | Batch 500 | Loss 0.3144\n",
            "\tEpoch 9 | Batch 1000 | Loss 0.2631\n",
            "\tEpoch 9 | Batch 1500 | Loss 0.0606\n",
            "\tEpoch 9 | Batch 2000 | Loss 0.1202\n",
            "\tEpoch 9 | Batch 2500 | Loss 0.0333\n",
            "\tEpoch 9 | Batch 3000 | Loss 0.3272\n",
            "Epoch 9 | Loss 0.1366\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for batch_num, input_data in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = input_data\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_num % 500 == 0:\n",
        "            print('\\tEpoch %d | Batch %d | Loss %6.4f' % (epoch, batch_num, loss.item()))\n",
        "\n",
        "    print('Epoch %d | Loss %6.4f' % (epoch, sum(losses)/len(losses)))\n",
        "\n",
        "torch.save(model, student_id + '_submission.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed922190",
      "metadata": {
        "id": "ed922190"
      },
      "source": [
        "## Part 11\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "fc906220",
      "metadata": {
        "id": "fc906220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb19b30-57a5-4f6c-b153-78874c14b420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8065\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "model.eval()\n",
        "\n",
        "with open(student_id + '_submission.csv', 'w') as f:\n",
        "\n",
        "    fieldnames = ['ImageId', 'Prediction', 'Label']\n",
        "\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames, lineterminator = '\\n')\n",
        "    writer.writeheader()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for x, t in test_loader:\n",
        "\n",
        "            x = x.to(device).float()\n",
        "            output = model(x).argmax(dim=1)\n",
        "\n",
        "            for y,l in zip(output, t):\n",
        "\n",
        "                writer.writerow({fieldnames[0]: (total+1),\n",
        "                                 fieldnames[1]: y.item(),\n",
        "                                 fieldnames[2]: l.item()})\n",
        "\n",
        "                total += 1\n",
        "                if y.item() == l.item():\n",
        "                    correct += 1\n",
        "\n",
        "    print('Accuracy: %6.4f' % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "cc305626",
      "metadata": {
        "id": "cc305626"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}