{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe4fc70b",
      "metadata": {
        "id": "fe4fc70b"
      },
      "source": [
        "# Assignment 3 -- Transformer\n",
        "\n",
        "## Goal\n",
        "\n",
        "1. Learning to design an Transformer architecture.\n",
        "\n",
        "2. Learning to implement and realize single (and multi-head) attention mechansim.\n",
        "\n",
        "3. Learning to implement the position-wise feedforward (FFN) Layer.\n",
        "\n",
        "\n",
        "## Score\n",
        "\n",
        "1. Position-wise feedforward (FFN) Layer 20%\n",
        "\n",
        "2. Single-head attention 20% (Bonus: multi-head attention 20%)\n",
        "\n",
        "3. Transformer architecture 20% (Please note that you can modify part 9)\n",
        "\n",
        "4. Model size 15%:\n",
        "\n",
        "* 10%: If your model size is smaller than **1MB**, you will get 10%. Otherwise, no points will be awarded.\n",
        "* 5%:  The remaining 5% will depend on your ranking within the class.\n",
        "\n",
        "5. Model accuracy 15%:\n",
        "\n",
        "* 10%: If your accuracy is higher than **57%**, you will get 10%. Otherwise, no points will be awarded.\n",
        "* 5%:  The remaining 5% will depend on your ranking within the class.\n",
        "\n",
        "6. Model accuracy on another dataset 10%: it will depand on your ranking within the class.\n",
        "\n",
        "\n",
        "## Rule\n",
        "\n",
        "1. You can use all torch and einops functions for your implementations.\n",
        "\n",
        "2. Please still avoid using other modules not mentioned above. Contact TA if you have questions about callable library.\n",
        "\n",
        "3. Please do NOT attempt to modify the sections `DO NOT MODIFY`.\n",
        "\n",
        "## Submission\n",
        "\n",
        "Upload your files to NTU Cool.\n",
        "* This .ipynb file: Please rename this file with the format (DL_HW3_StudentID.ipynb)\n",
        "* Model : .pt file\n",
        "* Output: .csv file\n",
        "\n",
        "Deadline: 5/20 midnight (23:59)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d37e4aa",
      "metadata": {
        "id": "5d37e4aa"
      },
      "source": [
        "Please fill your student ID number below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce70027f",
      "metadata": {
        "id": "ce70027f"
      },
      "outputs": [],
      "source": [
        "# Please fill your student ID number\n",
        "student_id = 'xxxxx'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc0d47f",
      "metadata": {
        "id": "abc0d47f"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Import necessary library\n",
        "\n",
        "`DO NOT MODIFY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e5db5d",
      "metadata": {
        "id": "67e5db5d"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.io import loadmat\n",
        "\n",
        "# Optimizer\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "# Pre-processing\n",
        "import torchvision.transforms as trns\n",
        "from PIL import Image\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "# For Transformer\n",
        "# you may need using `pip install einops` to install einops\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c316e796",
      "metadata": {
        "id": "c316e796"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Global variables.\n",
        "\n",
        "If following code used these variables, please keep them when you modify the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e882625f",
      "metadata": {
        "id": "e882625f"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_classes = 10\n",
        "input_size = (32, 32, 3)\n",
        "patch_size = 4\n",
        "num_epoch = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293110f1",
      "metadata": {
        "id": "293110f1"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Create dataloader with pre-processing of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f008c646",
      "metadata": {
        "id": "f008c646"
      },
      "outputs": [],
      "source": [
        "# Create train/test transforms\n",
        "train_transform = trns.Compose([\n",
        "    trns.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = trns.Compose([\n",
        "    trns.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create train/test datasets with pre-processing\n",
        "# The dataset will automatic download if does not exist\n",
        "data_train = datasets.CIFAR10(root='./dataset/', train=True, transform=train_transform, download=True)\n",
        "data_test = datasets.CIFAR10(root='./dataset/', train=False, transform=test_transform, download=True)\n",
        "\n",
        "# Create train/test dataloader for datasets with  pre-processing\n",
        "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(data_test,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b4d224",
      "metadata": {
        "id": "e8b4d224"
      },
      "source": [
        "## Part 4\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Convert image to token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fa91e5",
      "metadata": {
        "id": "a0fa91e5"
      },
      "outputs": [],
      "source": [
        "class myTokenization(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, patch_size, channels):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        patch_dim = patch_size * patch_size * channels\n",
        "\n",
        "        self.to_patch_tokens = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.to_patch_tokens(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e436d608",
      "metadata": {
        "id": "e436d608"
      },
      "source": [
        "## Part 5\n",
        "\n",
        "Please implement the following formula for the position-wise feedforward layer (FFN).\n",
        "\n",
        "$FFN(x) = max(0, x W_1 + b_1)W_2 + b_2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7acb2eb",
      "metadata": {
        "id": "b7acb2eb"
      },
      "outputs": [],
      "source": [
        "class myFFN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # your implementation\n",
        "        # example: single linear layer\n",
        "        # self.ffn = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # your implementation\n",
        "        # example: single linear layer\n",
        "        # out = self.ffn(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190eb6d7",
      "metadata": {
        "id": "190eb6d7"
      },
      "source": [
        "## Part 6\n",
        "\n",
        "Please implement the scaled dot-product attention (single head), including the masking (setting to −∞).\n",
        "\n",
        "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
        "\n",
        "_Bonus: please implement multi-head attention:_\n",
        "\n",
        "$MutliHead(Q, K, V) = Concate(head_1, ..., head_h)W^O$, where $head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0535341f",
      "metadata": {
        "id": "0535341f"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1JF7t1r3s2YBaaESo2UqnOn4ZGXpLSBZC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e448212",
      "metadata": {
        "id": "4e448212"
      },
      "outputs": [],
      "source": [
        "class myAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, heads, head_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # your implementation\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # your implementation\n",
        "        # Please implement scaled dot-product attention here (bonus: multi-head attention)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e419a4",
      "metadata": {
        "id": "74e419a4"
      },
      "source": [
        "## Part 7\n",
        "\n",
        "Please design your Transformer architecture with your implementation of {myFFN, myAttention}.\n",
        "\n",
        "You could decide number of layers, number of hidden neurons of each layer, activation function of each layer to design your Transformer.\n",
        "\n",
        "Please notice that your score will depands on both size and accuracy of your model.\n",
        "\n",
        "Here are the architectures of Transformer and ViT for your reference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d638e9df",
      "metadata": {
        "id": "d638e9df"
      },
      "source": [
        "### Transformer\n",
        "![picture](https://drive.google.com/uc?id=1_M6HgPKyInm1bjGQ81QiqIDR5BDAHXZs)\n",
        "\n",
        "### ViT\n",
        "![picture](https://drive.google.com/uc?id=18E82sYrhth9uJAF6ILGnmZlZ5tIcIUIo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5752f8",
      "metadata": {
        "id": "7c5752f8"
      },
      "outputs": [],
      "source": [
        "class myTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, heads, dim_head, mlp_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # your implementation\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # your implementation\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dd4d67",
      "metadata": {
        "id": "d5dd4d67"
      },
      "source": [
        "## Part 8\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Construct Vision Transformer by implemented Transformer and FFN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb0fc9f",
      "metadata": {
        "id": "feb0fc9f"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1TKPJyIkGXSnHN9BTJA5XTZ3QtyORqi89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee559b3",
      "metadata": {
        "id": "7ee559b3"
      },
      "outputs": [],
      "source": [
        "class myViT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, patch_size, hidden_dim, heads, head_dim, mlp_dim, num_classes):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        image_height, image_width, channels = input_size\n",
        "        patch_height, patch_width = patch_size, patch_size\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = patch_height * patch_width * channels\n",
        "\n",
        "        self.to_input_token = myTokenization(hidden_dim, patch_size, channels)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, hidden_dim))\n",
        "\n",
        "        self.transformer = myTransformer(hidden_dim, heads, head_dim, mlp_dim)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # convert image to token embeddings\n",
        "        x = self.to_input_token(x)\n",
        "\n",
        "        # concatenate cls token and input token and add positional embedding\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # transformer\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # cls token for prediction\n",
        "        x = x[:, 0]\n",
        "\n",
        "        return self.mlp_head(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad19f7e",
      "metadata": {
        "id": "0ad19f7e"
      },
      "source": [
        "## Part 9\n",
        "\n",
        "You could adjust `hidden_dim`, `heads`, `head_dim`, and `mlp_dim` to construct your ViT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608127aa",
      "metadata": {
        "id": "608127aa"
      },
      "outputs": [],
      "source": [
        "model = myViT(\n",
        "    input_size = input_size,\n",
        "    patch_size = patch_size,\n",
        "    hidden_dim = 64,\n",
        "    heads = 4,\n",
        "    head_dim = 64,\n",
        "    mlp_dim = 64,\n",
        "    num_classes = num_classes).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a7d1da",
      "metadata": {
        "id": "41a7d1da"
      },
      "source": [
        "## Part 10\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Multiclass cross-entropy los"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e16dbca",
      "metadata": {
        "id": "2e16dbca"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1522cb8a",
      "metadata": {
        "id": "1522cb8a"
      },
      "source": [
        "## Part 11\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5df22d",
      "metadata": {
        "id": "3c5df22d"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d20afbce",
      "metadata": {
        "id": "d20afbce"
      },
      "source": [
        "## Part 12\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008bdc73",
      "metadata": {
        "id": "008bdc73"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for batch_num, input_data in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = input_data\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_num % 500 == 0:\n",
        "            print('\\tEpoch %d | Batch %d | Loss %6.4f' % (epoch, batch_num, loss.item()))\n",
        "\n",
        "    print('Epoch %d | Loss %6.4f' % (epoch, sum(losses)/len(losses)))\n",
        "\n",
        "torch.save(model, student_id + '_submission.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed922190",
      "metadata": {
        "id": "ed922190"
      },
      "source": [
        "## Part 8\n",
        "\n",
        "`DO NOT MODIFY`\n",
        "\n",
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc906220",
      "metadata": {
        "id": "fc906220"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "model.eval()\n",
        "\n",
        "with open(student_id + '_submission.csv', 'w') as f:\n",
        "\n",
        "    fieldnames = ['ImageId', 'Prediction', 'Label']\n",
        "\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames, lineterminator = '\\n')\n",
        "    writer.writeheader()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for x, t in test_loader:\n",
        "\n",
        "            x = x.to(device).float()\n",
        "            output = model(x).argmax(dim=1)\n",
        "\n",
        "            for y,l in zip(output, t):\n",
        "\n",
        "                writer.writerow({fieldnames[0]: (total+1),\n",
        "                                 fieldnames[1]: y.item(),\n",
        "                                 fieldnames[2]: l.item()})\n",
        "\n",
        "                total += 1\n",
        "                if y.item() == l.item():\n",
        "                    correct += 1\n",
        "\n",
        "    print('Accuracy: %6.4f' % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066ebf13",
      "metadata": {
        "id": "066ebf13"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
